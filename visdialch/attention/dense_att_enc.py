import torch
import torch.nn as nn
import torch.nn.functional as F
from visdialch.utils import DynamicRNN_v2
import numpy as np
import pdb

class DenseCoAttLayer(nn.Module):
      '''
      This layer implements the image-feature/content-feature dense co-attention mechanisum proposed by:
       "Improved Fusion of Visual and Language Representations by Dense Symmetric Co-Attention for Visual Question Answering"
       (https://arxiv.org/pdf/1804.00775.pdf)
      '''
      def __init__(self, word_emb_dim=300, lstm_hidden_dim=512, lstm_num_layers=2, img_fea_dim=2048, dropout=0., sub_maps=4, K=2, stack_depth=2):
          '''
          Args:
            @param word_emb_dim (int): dimension of the word embedding.
            @param lstm_hidden_dim (int): dimension of the LSTM hidden state.
            @param img_feature_dim (int): dimension of the image features.
            @param dropout (float): the dropout rate of LSTM
            @param K (int): #nowhere-to-attend locations
            @param stack_depth (int): #stacked co-attention layers
          '''
          super(DenseCoAttLayer, self).__init__()
          self.word_emb_dim = word_emb_dim
          self.lstm_hidden_dim = lstm_hidden_dim
          self.img_fea_dim = img_fea_dim
          self.fea_dim = 2 * lstm_hidden_dim #specific internal feature dimension in co-attention layer. (Bi-LSTM hidden state dimension)
          self.imfea_reducer = nn.Linear(img_fea_dim, self.fea_dim) #reduce the dimension of the image feature to be 
                                                                         #the same as the bidirectional LSTM output.
          self.content_lstm = nn.LSTM(
               input_size=word_emb_dim, 
               hidden_size=lstm_hidden_dim,
               num_layers=lstm_num_layers, 
               dropout=dropout,
               batch_first=True,
               bidirectional=True
          )
          self.content_lstm = DynamicRNN_v2(self.content_lstm) #Bi-LSTM to generate Q[q_1, q_2, ... , q_N]
          assert self.fea_dim % sub_maps == 0
          self.sub_proj_dim = self.fea_dim //sub_maps 
          self.sub_maps = sub_maps
          self.K = K 
          self.stack_depth = stack_depth
          self.sub_proj_Wv = nn.ParameterList([nn.Parameter(torch.zeros((sub_maps, self.sub_proj_dim, self.fea_dim))) for i in range(stack_depth)])
          self.sub_proj_Wq = nn.ParameterList([nn.Parameter(torch.zeros((sub_maps, self.sub_proj_dim, self.fea_dim))) for i in range(stack_depth)])
          self.non_att_Mv = nn.ParameterList([nn.Parameter(torch.zeros((K, self.fea_dim))) for i in range(stack_depth)]) #Nowhere-to-attend location for image feature
          self.non_att_Mq = nn.ParameterList([nn.Parameter(torch.zeros((K, self.fea_dim))) for i in range(stack_depth)]) #Nowhere-to-attend location for question feature

          self.fc = nn.ModuleList([nn.ModuleList([nn.Linear(2*self.fea_dim, self.fea_dim), nn.Linear(2*self.fea_dim, self.fea_dim)]) for i in range(stack_depth)]) #fully connected layers
          
          #initilize the weights
          for weight in [self.imfea_reducer.weight]:
              nn.init.kaiming_uniform_(weight)
          for i in range(stack_depth):
              nn.init.kaiming_uniform_(self.non_att_Mv[i])
              nn.init.kaiming_uniform_(self.non_att_Mq[i])
              nn.init.kaiming_uniform_(self.sub_proj_Wv[i])
              nn.init.kaiming_uniform_(self.sub_proj_Wq[i])
              for layer in self.fc[i]:
                  nn.init.kaiming_uniform_(layer.weight)
                  nn.init.constant_(layer.bias, 0.)
          nn.init.constant_(self.imfea_reducer.bias, 0.)
 
      def get_feature_dim(self):
          return self.fea_dim

      def forward(self, content, content_len, img_fea):
          '''
          Args:
            @param content (tensor): the tokenized sentence. 
                           QA-history (batch, rounds, 2*max_sequence_length, self.word_emb_dim)
                           Question history/Answer options: (batch, rounds, max_sequence_length, self.word_emb_dim)
            @param content_len (tensor): the length of the content (batch, rounds)
            @param img_fea (tensor): the image features (batch, region_num, self.img_feature_dim)
          Return:
            Q (tensor): question representation generated by co-attention stack (batch, rounds, 2*max_sequence_length, 2*self.lstm_hidden_size)
            V (tensor): image representation generated by co-attention stack (batch, rounds, region_num, 2*self.lstm_hidden_size)
          '''
          batch, rounds, max_qa_len, word_emb_dim = content.size() #max_qa_len = 2*max_sequence_length
          assert self.word_emb_dim == word_emb_dim
          _, region_num, img_fea_dim = img_fea.size()
          assert self.img_fea_dim == img_fea_dim
          
          Q, (s0, _) = self.content_lstm(content.view(batch*rounds, max_qa_len, -1), content_len.view(-1)) #Q (batch*rounds, max_qa_len, self.fea_dim)
                                                                                                #s0 (2, batch*rounds, self.lstm_hidden_dim)
          s0 = s0.transpose(0,1).contiguous().view(batch*rounds, -1)
          Q = Q.view(batch, rounds, max_qa_len, -1) #(batch, rounds, max_qa_len, self.fea_dim)
          V = self.imfea_reducer(img_fea) #(batch, region_num, self.fea_dim)
          V = V.unsqueeze(1).repeat(1, rounds, 1, 1) #batch, rounds, region_num, self.fea_dim

          for i in range(self.stack_depth):
             Q_t = torch.zeros((batch, rounds, max_qa_len+self.K, self.fea_dim)).cuda() #contatiner
             Q_t[:,:,0:max_qa_len] = Q
             Q_t = Q_t.view(batch*rounds, max_qa_len+self.K, -1) #(batch*rounds, max_qa_len+self.K, self.fea_dim)
             idx = content_len.view(-1,1) #(batch*rounds,1)
             idx = torch.cat([idx + k for k in range(self.K)], -1) #(batch*rounds, K)
             j = torch.arange(batch*rounds).long().view(-1,1) #(batch*rounds, 1)
             Q_t[j, idx] = self.non_att_Mq[i] #append the nowhere-to-attend position right after the meaningfull content 
                                              #(batch*rounds, max_qa_len+self.K, self.fea_dim)
             Q_t = Q_t.view(batch, rounds, max_qa_len+self.K, -1) #(batch, rounds, max_qa_len+self.K, self.fea_dim)

             non_att_Mv = self.non_att_Mv[i].view(1,1,self.K, -1).expand(batch, rounds, -1, -1) #(batch, rounds, self.K, self.fea_dim)
             V_t = torch.cat((V, non_att_Mv), 2) #(batch, rounds, region_num+self.K, self.fea_dim)

             Q_s = Q_t.transpose(2,3).unsqueeze(2) #(batch, rounds, 1, self.fea_dim, max_qa_len+self.K)
             V_s = V_t.transpose(2,3).unsqueeze(2) #(batch, rounds, 1, self.fea_dim, region_num+self.K)

             #build the attention matrix
             W_q = torch.matmul(self.sub_proj_Wq[i], Q_s) #(batch, rounds, self.sub_maps, self.sub_proj_dim, max_qa_len+self.K)
             W_v = torch.matmul(self.sub_proj_Wv[i], V_s) #(batch, rounds, self.sub_maps, self.sub_proj_dim, region_num+self.K)
             A_i = torch.matmul(W_v.transpose(3,4), W_q) #(batch, rounds, self.sub_maps, region_num+self.K, max_qa_len+self.K)
             A_q = F.softmax(A_i/np.sqrt(self.sub_maps), -1)#matrix indicating how a specific region attending to the content. 
                                                            #(batch, rounds, self.sub_maps, region_num+self.K, max_qa_len+self.K)
             A_v = F.softmax(A_i.transpose(3,4)/np.sqrt(self.sub_maps), -1)#matrix indicating how a specific word attending to the regions
                                                                         #(batch, rounds, self.sub_maps, max_qa_len+self.K, region_num+self.K)
             A_q = A_q.mean(2) #(batch, rounds, region_num+self.K, max_qa_len+self.K)
             A_v = A_v.mean(2) #(batch, rounds, max_qa_len+self.K, region_num+self.K)
            
             #content aware region representation
             Q_l = torch.matmul(A_q[:,:,0:region_num], Q_t) #(batch, rounds, region_num, self.fea_dim)
             #region aware content representation
             V_l = torch.matmul(A_v[:,:,0:max_qa_len], V_t) #(batch, rounds, max_qa_len, self.fea_dim)

             V_f = torch.cat([V, Q_l], -1) #(batch, rounds, region_num, self.fea_dim*2)
             Q_f = torch.cat([Q, V_l], -1) #(batch, rounds, max_qa_len, self.fea_dim*2)

             #the final fully connected and skip layers
             V = F.relu(self.fc[i][0](V_f)) + V #(batch, rounds, region_num, self.fea_dim)
             Q = F.relu(self.fc[i][1](Q_f)) + Q #(batch, rounds, max_qa_len, self.fea_dim)

          return Q, V

class DenseCoAttEncoder(nn.Module):
      '''
      DenseCoAttEncoder encode questions, dialog history, anwsor options with DenseCoAttLayer.
      '''
      def __init__(self, config):
          '''
          @param config : configuration file
          '''
          super(DenseCoAttEncoder, self).__init__()
          self.word_emb_dim = config['word_embedding_size']
          self.lstm_hidden_dim = config['content_lstm_hidden_size']
          self.lstm_num_layers = config['content_lstm_num_layers']
          self.img_fea_dim = config['img_feature_size']
          self.dropout = config['content_lstm_dropout']
          self.sub_maps = config['sub_maps']
          self.K = config.get('K', 2)
          self.stack_depth = config['stack_depth']
          self.reason_mode = config['reason_mode']
          self.att_layer = DenseCoAttLayer(self.word_emb_dim, self.lstm_hidden_dim, 
                                           self.lstm_num_layers, self.img_fea_dim, 
                                           self.dropout, self.sub_maps, self.K, self.stack_depth)
          self.mlp_Q = nn.Sequential(nn.Linear(self.att_layer.get_feature_dim(), self.lstm_hidden_dim), 
                                     nn.ReLU(), 
                                     nn.Linear(self.lstm_hidden_dim, 1)
                       ) #self-attention MLP for content feature
          self.mlp_V = nn.Sequential(nn.Linear(self.att_layer.get_feature_dim(), self.lstm_hidden_dim), 
                                     nn.ReLU(), 
                                     nn.Linear(self.lstm_hidden_dim, 1)
                       ) #self-attention MLP for image feature
          def init_weights(l):
              if type(l) == nn.Linear:
                 nn.init.kaiming_uniform_(l.weight)
          self.mlp_Q.apply(init_weights)
          self.mlp_V.apply(init_weights)

          self.att_fea_dim = self.att_layer.get_feature_dim()
          if self.reason_mode == 'lstm':
            self.hist_lstm = nn.LSTM(
                input_size=self.att_fea_dim, 
                hidden_size=self.att_fea_dim,
                num_layers=2, batch_first=True, bidirectional=False
            ) #the LSTM used to propagate the dialog histroy
            self.hist_rnn = DynamicRNN_v2(self.hist_lstm)

      def get_feature_dim(self):
          return self.att_fea_dim
      
      def forward(self, content, content_len, img_fea, content_type):    
           '''
           Args:
             @param content (tensor): the tokenized sentence, content can be question history, answer options, dialog QA history.
                                     (batch, rounds, max_sequence_length, word_emb_dim) for question history and answer options
                                     (batch, rounds, 2*max_sequence_length, word_emb_dim) for dialog history
                                      For each batch, rounds=100 to encode all answer candidates, 
                                                      round = 10 to encode all dialog QA history or question history
             @param content_len (tensor): the length of the content (batch, rounds)
             @param img_fea (tensor): the image features (batch, region_num, self.img_feature_dim)
             @param  content_type (str): type of the content, one in ['question', 'dialog', 'answer']
             Return:
             s_QV (tensor): (batch, att_fea_dim) for 'question'/'dialog' case, a att_fea_dim vector for each batch as the history summary.
                            (batch, rounds, att_fea_dim) for 'answer' case, a att_fea_dim vector for each candidate answer
           '''
           batch, rounds, max_seq_len, word_emb_dim = content.size()
           _, region_num, _ = img_fea.size()
           Q, V = self.att_layer(content, content_len, img_fea) #Q (batch, rounds, max_seq_len, att_fea_dim)
                                                                #V (batch, rounds, region_num, att_fea_dim)
           #compute the self-attention weight
           att_Q = F.softmax(self.mlp_Q(Q), 2) #(batch, rounds, max_seq_len, 1)
           att_V = F.softmax(self.mlp_V(V), 2) #(batch, rounds, region_num, 1)

           #compute the weight-sum feature
           s_Q = (att_Q * Q).sum(2) #(batch, rounds, att_fea_dim)
           s_V = (att_V * V).sum(2) #(batch, rounds, att_fea_dim)
           s_QV = s_Q + s_V #use the simple summation under the intuition that the V->Q and Q->V have equal importance.
                            #(batch, rounds, att_fea_dim)
           if self.reason_mode == 'lstm':
             if content_type == 'question' or content_type == 'dialog':
                #summarize the question/dialog history by LSTM 
                lens = torch.LongTensor([rounds]*batch).cuda() #need explicitly push to GPU? 
                '''
                  _, (s_QV, _) = self.hist_rnn(s_QV, lens) # new s_QV: (2, batch, att_fea_dim) 
                  s_QV = s_QV.transpose(0,1).contiguous().view(batch, -1) #(batch, 2*att_fea_dim)
                '''
                s_QV, (_, _) = self.hist_rnn(s_QV, lens) #new s_QV: (batch, rounds, att_fea_dim)

           return s_QV
